#!/usr/bin/env python3
"""
–°–∏—Å—Ç–µ–º–∞ backup –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞ strain_collection_new

–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç:
- –ü–æ–ª–Ω—ã–µ backup'—ã —Å –¥–∞–Ω–Ω—ã–º–∏
- Schema-only backup'—ã
- –°–∂–∞—Ç–∏–µ backup'–æ–≤
- –†–æ—Ç–∞—Ü–∏—é —Å—Ç–∞—Ä—ã—Ö backup'–æ–≤
- –í–∞–ª–∏–¥–∞—Ü–∏—é backup'–æ–≤
- –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–π
"""

import argparse
import datetime
import gzip
import json
import logging
import os
import shutil
import subprocess
import sys
from pathlib import Path
from typing import Any, Dict, List, Optional

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ Django –ø—Ä–æ–µ–∫—Ç—É
project_root = Path(__file__).resolve().parent.parent.parent
sys.path.append(str(project_root / "backend"))
os.environ.setdefault(
    "DJANGO_SETTINGS_MODULE", "strain_tracker_project.settings"
)

import django  # noqa: E402

django.setup()

from django.conf import settings  # noqa: E402
from django.db import connection  # noqa: E402


class DatabaseBackup:
    """–ö–ª–∞—Å—Å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è backup'–∞–º–∏ PostgreSQL –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""

    def __init__(self, backup_dir: str = None):
        self.backup_dir = Path(
            backup_dir
            or os.path.join(os.path.dirname(__file__), "..", "backups")
        )
        self.backup_dir.mkdir(exist_ok=True)

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        self.setup_logging()

        # –ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –ë–î –∏–∑ Django
        self.db_config = settings.DATABASES["default"]
        self.db_name = self.db_config["NAME"]
        self.db_user = self.db_config["USER"]
        self.db_password = self.db_config["PASSWORD"]
        self.db_host = self.db_config["HOST"] or "localhost"
        self.db_port = self.db_config["PORT"] or "5432"

    def setup_logging(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"""
        log_file = self.backup_dir / "backup.log"
        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - %(levelname)s - %(message)s",
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler(sys.stdout),
            ],
        )
        self.logger = logging.getLogger(__name__)

    def get_timestamp(self) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ timestamp –¥–ª—è –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞"""
        return datetime.datetime.now().strftime("%Y%m%d_%H%M%S")

    def get_database_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        try:
            with connection.cursor() as cursor:
                # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–∞–±–ª–∏—Ü
                cursor.execute(
                    """
                    SELECT COUNT(*) FROM information_schema.tables 
                    WHERE table_schema = 'public' AND table_type = 'BASE TABLE'
                """
                )
                tables_count = cursor.fetchone()[0]

                # –†–∞–∑–º–µ—Ä –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
                cursor.execute(
                    "SELECT pg_size_pretty(pg_database_size(%s))",
                    [self.db_name],
                )
                db_size = cursor.fetchone()[0]

                # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π –≤ –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü–∞—Ö
                stats = {}
                for table in [
                    "collection_manager_strain",
                    "collection_manager_sample",
                    "collection_manager_storage",
                ]:
                    try:
                        cursor.execute(f"SELECT COUNT(*) FROM {table}")
                        stats[table] = cursor.fetchone()[0]
                    except Exception:
                        stats[table] = 0

                return {
                    "timestamp": datetime.datetime.now().isoformat(),
                    "database": self.db_name,
                    "tables_count": tables_count,
                    "database_size": db_size,
                    "records": stats,
                }
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ë–î: {e}")
            return {}

    def create_backup(
        self, backup_type: str = "full", compress: bool = True
    ) -> Optional[Path]:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ backup'–∞ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö

        Args:
            backup_type: 'full' - –ø–æ–ª–Ω—ã–π backup, 'schema' - —Ç–æ–ª—å–∫–æ —Å—Ö–µ–º–∞
            compress: —Å–∂–∏–º–∞—Ç—å –ª–∏ backup
        """
        timestamp = self.get_timestamp()

        if backup_type == "full":
            filename = f"strain_collection_full_{timestamp}.sql"
        elif backup_type == "schema":
            filename = f"strain_collection_schema_{timestamp}.sql"
        else:
            raise ValueError("backup_type –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å 'full' –∏–ª–∏ 'schema'")

        backup_path = self.backup_dir / filename

        try:
            self.logger.info(f"–°–æ–∑–¥–∞–Ω–∏–µ {backup_type} backup: {filename}")

            # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã pg_dump
            cmd = [
                "pg_dump",
                f"--host={self.db_host}",
                f"--port={self.db_port}",
                f"--username={self.db_user}",
                "--no-password",
                "--verbose",
            ]

            if backup_type == "schema":
                cmd.append("--schema-only")

            cmd.append(self.db_name)

            # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è –ø–∞—Ä–æ–ª—è
            env = os.environ.copy()
            env["PGPASSWORD"] = self.db_password

            # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ backup'–∞
            self.logger.info(
                f"–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã: {' '.join(cmd[:-1])} [DB_NAME]"
            )

            with open(backup_path, "w") as f:
                result = subprocess.run(
                    cmd, stdout=f, stderr=subprocess.PIPE, env=env, text=True
                )

            if result.returncode != 0:
                self.logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è backup: {result.stderr}")
                if backup_path.exists():
                    backup_path.unlink()
                return None

            # –°–∂–∞—Ç–∏–µ backup'–∞
            if compress:
                compressed_path = backup_path.with_suffix(".sql.gz")
                self.logger.info(f"–°–∂–∞—Ç–∏–µ backup: {compressed_path.name}")

                with open(backup_path, "rb") as f_in:
                    with gzip.open(compressed_path, "wb") as f_out:
                        shutil.copyfileobj(f_in, f_out)

                backup_path.unlink()  # –£–¥–∞–ª—è–µ–º –Ω–µ—Å–∂–∞—Ç—ã–π —Ñ–∞–π–ª
                backup_path = compressed_path

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            self.save_backup_metadata(backup_path, backup_type)

            self.logger.info(f"Backup —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω: {backup_path}")
            return backup_path

        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è backup: {e}")
            if backup_path.exists():
                backup_path.unlink()
            return None

    def save_backup_metadata(self, backup_path: Path, backup_type: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö backup'–∞"""
        metadata = {
            "backup_file": backup_path.name,
            "backup_type": backup_type,
            "creation_time": datetime.datetime.now().isoformat(),
            "database_stats": self.get_database_stats(),
            "file_size": backup_path.stat().st_size,
            "compressed": backup_path.suffix == ".gz",
        }

        metadata_path = backup_path.with_suffix(".json")
        with open(metadata_path, "w", encoding="utf-8") as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)

        self.logger.info(f"–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {metadata_path.name}")

    def list_backups(self) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –≤—Å–µ—Ö backup'–æ–≤"""
        backups = []

        for backup_file in self.backup_dir.glob("strain_collection_*.sql*"):
            if backup_file.suffix in [".sql", ".gz"]:
                metadata_file = backup_file.with_suffix(".json")

                if metadata_file.exists():
                    try:
                        with open(metadata_file, "r", encoding="utf-8") as f:
                            metadata = json.load(f)
                        backups.append(metadata)
                    except Exception as e:
                        self.logger.warning(
                            f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö {metadata_file}: {e}"
                        )
                else:
                    # –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ñ–∞–π–ª–æ–≤ –±–µ–∑ –Ω–∏—Ö
                    stat = backup_file.stat()
                    backups.append(
                        {
                            "backup_file": backup_file.name,
                            "backup_type": "unknown",
                            "creation_time": datetime.datetime.fromtimestamp(
                                stat.st_mtime
                            ).isoformat(),
                            "file_size": stat.st_size,
                            "compressed": backup_file.suffix == ".gz",
                        }
                    )

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ —Å–æ–∑–¥–∞–Ω–∏—è (–Ω–æ–≤—ã–µ –ø–µ—Ä–≤—ã–º–∏)
        backups.sort(key=lambda x: x["creation_time"], reverse=True)
        return backups

    def cleanup_old_backups(self, keep_days: int = 30, keep_count: int = 10):
        """
        –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö backup'–æ–≤

        Args:
            keep_days: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è
            keep_count: –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ backup'–æ–≤ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        """
        backups = self.list_backups()
        cutoff_date = datetime.datetime.now() - datetime.timedelta(
            days=keep_days
        )

        removed_count = 0
        for i, backup in enumerate(backups):
            if i < keep_count:  # –í—Å–µ–≥–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                continue

            backup_date = datetime.datetime.fromisoformat(
                backup["creation_time"]
                .replace("Z", "+00:00")
                .replace("+00:00", "")
            )

            if backup_date < cutoff_date:
                backup_path = self.backup_dir / backup["backup_file"]
                metadata_path = backup_path.with_suffix(".json")

                try:
                    if backup_path.exists():
                        backup_path.unlink()
                    if metadata_path.exists():
                        metadata_path.unlink()

                    self.logger.info(
                        f"–£–¥–∞–ª–µ–Ω —Å—Ç–∞—Ä—ã–π backup: {backup['backup_file']}"
                    )
                    removed_count += 1

                except Exception as e:
                    self.logger.error(
                        f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è backup {backup['backup_file']}: {e}"
                    )

        self.logger.info(f"–£–¥–∞–ª–µ–Ω–æ —Å—Ç–∞—Ä—ã—Ö backup'–æ–≤: {removed_count}")

    def validate_backup(self, backup_file: str) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è backup —Ñ–∞–π–ª–∞"""
        backup_path = self.backup_dir / backup_file

        if not backup_path.exists():
            self.logger.error(f"Backup —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {backup_file}")
            return False

        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ —Ñ–∞–π–ª–∞
            if backup_path.suffix == ".gz":
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ gzip —Ñ–∞–π–ª–∞
                with gzip.open(backup_path, "rt") as f:
                    # –ß–∏—Ç–∞–µ–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫
                    for i, line in enumerate(f):
                        if i > 10:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 10 —Å—Ç—Ä–æ–∫
                            break
                        if not line.strip():
                            continue
                        # –û–∂–∏–¥–∞–µ–º SQL –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∏–ª–∏ –∫–æ–º–∞–Ω–¥—ã
                        if not (
                            line.startswith("--")
                            or line.startswith("SET")
                            or line.startswith("CREATE")
                            or line.startswith("INSERT")
                        ):
                            self.logger.warning(
                                f"–ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ –≤ backup: {line[:50]}..."
                            )
            else:
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—ã—á–Ω–æ–≥–æ SQL —Ñ–∞–π–ª–∞
                with open(backup_path, "r") as f:
                    first_lines = [f.readline() for _ in range(10)]
                    if not any(
                        "PostgreSQL" in line or "pg_dump" in line
                        for line in first_lines
                    ):
                        self.logger.warning(
                            "Backup –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ PostgreSQL"
                        )

            self.logger.info(f"Backup —Ñ–∞–π–ª –≤–∞–ª–∏–¥–µ–Ω: {backup_file}")
            return True

        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ backup {backup_file}: {e}")
            return False


def main():
    parser = argparse.ArgumentParser(
        description="–°–∏—Å—Ç–µ–º–∞ backup –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö strain_collection"
    )
    parser.add_argument(
        "action",
        choices=["create", "list", "cleanup", "validate"],
        help="–î–µ–π—Å—Ç–≤–∏–µ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è",
    )
    parser.add_argument(
        "--type",
        choices=["full", "schema"],
        default="full",
        help="–¢–∏–ø backup (—Ç–æ–ª—å–∫–æ –¥–ª—è create)",
    )
    parser.add_argument(
        "--no-compress",
        action="store_true",
        help="–ù–µ —Å–∂–∏–º–∞—Ç—å backup (—Ç–æ–ª—å–∫–æ –¥–ª—è create)",
    )
    parser.add_argument("--backup-dir", help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è backup'–æ–≤")
    parser.add_argument(
        "--keep-days",
        type=int,
        default=30,
        help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è backup'–æ–≤ (—Ç–æ–ª—å–∫–æ –¥–ª—è cleanup)",
    )
    parser.add_argument(
        "--keep-count",
        type=int,
        default=10,
        help="–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ backup'–æ–≤ (—Ç–æ–ª—å–∫–æ –¥–ª—è cleanup)",
    )
    parser.add_argument(
        "--file", help="–ò–º—è —Ñ–∞–π–ª–∞ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (—Ç–æ–ª—å–∫–æ –¥–ª—è validate)"
    )

    args = parser.parse_args()

    # –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞ backup
    backup = DatabaseBackup(args.backup_dir)

    if args.action == "create":
        result = backup.create_backup(
            backup_type=args.type, compress=not args.no_compress
        )
        if result:
            print(f"‚úÖ Backup —Å–æ–∑–¥–∞–Ω: {result}")
            sys.exit(0)
        else:
            print("‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è backup")
            sys.exit(1)

    elif args.action == "list":
        backups = backup.list_backups()
        if not backups:
            print("üìÇ Backup'—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        else:
            print("üìÇ –°–ø–∏—Å–æ–∫ backup'–æ–≤:")
            for i, b in enumerate(backups, 1):
                size_mb = b["file_size"] / (1024 * 1024)
                compressed = " (—Å–∂–∞—Ç)" if b.get("compressed") else ""
                print(f"{i:2d}. {b['backup_file']}{compressed}")
                print(f"    –î–∞—Ç–∞: {b['creation_time'][:19]}")
                print(f"    –¢–∏–ø: {b['backup_type']}, –†–∞–∑–º–µ—Ä: {size_mb:.1f} MB")
                if "database_stats" in b and "records" in b["database_stats"]:
                    records = b["database_stats"]["records"]
                    print(
                        f"    –î–∞–Ω–Ω—ã–µ: —à—Ç–∞–º–º—ã={records.get('collection_manager_strain', 0)}, "
                        f"–æ–±—Ä–∞–∑—Ü—ã={records.get('collection_manager_sample', 0)}"
                    )
                print()

    elif args.action == "cleanup":
        backup.cleanup_old_backups(args.keep_days, args.keep_count)
        print(
            f"‚úÖ –û—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ (—Ö—Ä–∞–Ω–µ–Ω–∏–µ: {args.keep_days} –¥–Ω–µ–π, –º–∏–Ω–∏–º—É–º: {args.keep_count} —Ñ–∞–π–ª–æ–≤)"
        )

    elif args.action == "validate":
        if not args.file:
            print("‚ùå –£–∫–∞–∂–∏—Ç–µ —Ñ–∞–π–ª –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Å –ø–æ–º–æ—â—å—é --file")
            sys.exit(1)

        if backup.validate_backup(args.file):
            print(f"‚úÖ Backup —Ñ–∞–π–ª –≤–∞–ª–∏–¥–µ–Ω: {args.file}")
        else:
            print(f"‚ùå Backup —Ñ–∞–π–ª –Ω–µ –≤–∞–ª–∏–¥–µ–Ω: {args.file}")
            sys.exit(1)


if __name__ == "__main__":
    main()
